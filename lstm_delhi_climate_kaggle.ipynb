{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Model to Predict Weather in New Delhi\n",
    "\n",
    "This notebook aims to forecast the mean temperature in New Delhi using 4 different features (mean temperature, humidity, wind speed and mean pressure).\n",
    "\n",
    "A Biderectional Long Short Term Memory model is used for this purpose. The dataset is a time series and the model built in this case uses the previous 30 days of data to forecast the mean temperature of the next day.\n",
    "\n",
    "In a bidirectional LSTM, we give the input from both right to left and from left to right."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing of Packages\n",
    "\n",
    "Here, we import the necessary packages such as numpy (for numerical and array calculations), pandas (data-handling) and torch (building of machine learning model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn   # neural network modules\n",
    "import torch.optim as optim   # optimization algorithms\n",
    "import torch.nn.functional as F   # functions without parameters like activation functions\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset   # dataset management, create batches\n",
    "import torchvision.datasets as datasets   # standard datasets on pytorch\n",
    "import torchvision.transforms as transforms   #transform datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "Set device to GPU if available, else, send to CPU. The data and model will be sent to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (1462, 5)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
    "\n",
    "print(f\"Full train dataset shape is {train_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   meantemp   humidity  wind_speed  meanpressure\n",
       "0  2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
       "1  2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
       "2  2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
       "3  2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
       "4  2013-01-05   6.000000  86.833333    3.700000   1016.500000"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data points briefly using a box plot. This helps us to visualise the data and identify outliers at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.plot(kind='box')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a closer look at the data to find outliers and if it is necessary to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1462.000000</td>\n",
       "      <td>1462.000000</td>\n",
       "      <td>1462.000000</td>\n",
       "      <td>1462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.495521</td>\n",
       "      <td>60.771702</td>\n",
       "      <td>6.802209</td>\n",
       "      <td>1011.104548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.348103</td>\n",
       "      <td>16.769652</td>\n",
       "      <td>4.561602</td>\n",
       "      <td>180.231668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.857143</td>\n",
       "      <td>50.375000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1001.580357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.714286</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>6.221667</td>\n",
       "      <td>1008.563492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.305804</td>\n",
       "      <td>72.218750</td>\n",
       "      <td>9.238235</td>\n",
       "      <td>1014.944901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>38.714286</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>42.220000</td>\n",
       "      <td>7679.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          meantemp     humidity   wind_speed  meanpressure\n",
       "count  1462.000000  1462.000000  1462.000000   1462.000000\n",
       "mean     25.495521    60.771702     6.802209   1011.104548\n",
       "std       7.348103    16.769652     4.561602    180.231668\n",
       "min       6.000000    13.428571     0.000000     -3.041667\n",
       "25%      18.857143    50.375000     3.475000   1001.580357\n",
       "50%      27.714286    62.625000     6.221667   1008.563492\n",
       "75%      31.305804    72.218750     9.238235   1014.944901\n",
       "max      38.714286   100.000000    42.220000   7679.333333"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove outliers using z-score (number of standard deviations by which the value of a raw score is above or below the mean value) and we set the limit to 3. As seen from the table above, this only applies to wind speed and mean pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "(1444, 5)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z_windspeed = np.abs(stats.zscore(train_dataset['wind_speed']))\n",
    "z_meanpressure = np.abs(stats.zscore(train_dataset['meanpressure']))\n",
    "\n",
    "z_data = [z_windspeed,z_meanpressure]\n",
    "z_list = []\n",
    "for x in z_data:\n",
    "    z_list.extend(np.where(x>3)[0].flatten().tolist())\n",
    "\n",
    "print(len(z_list))\n",
    "train_dataset = train_dataset.drop(index = z_list, axis=0)\n",
    "\n",
    "final_dataset = pd.DataFrame(train_dataset)\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'])\n",
    "\n",
    "final_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature set which is a numpy array containing the data for 4 features and for all rows.\n",
    "\n",
    "We then choose the mean temperature to be the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1444, 4)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set = final_dataset.to_numpy()[:,[1,2,3,4]]\n",
    "out_feature_index = 0 #meantemp\n",
    "feature_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.0, 84.5, 0.0, 1015.6666666666666],\n",
       "       [7.4, 92.0, 2.98, 1017.8],\n",
       "       [7.166666666666667, 87.0, 4.633333333333334, 1018.6666666666666],\n",
       "       ...,\n",
       "       [14.095238095238097, 89.66666666666667, 6.266666666666667,\n",
       "        1017.904761904762],\n",
       "       [15.052631578947368, 87.0, 7.325, 1016.1],\n",
       "       [10.0, 100.0, 0.0, 1016.0]], dtype=object)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the data within the range 0 to 1 using MinMaxScaler and create 2 datasets (one containing all the 4 features, one containing only the mean temperatures). For the output set, the reshape function is needed as the scaler expects a 2D array.\n",
    "\n",
    "Purpose of normalization is so that during gradient descent, the model will be able to converge better to a minimum. If the features had different ranges, their gradients will vary and the rate of change of the cost function with respect to each feature will be different. A unit change for a feature with smaller range will have a smaller impact on the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1444, 4)\n",
      "(1444, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_normalize = ['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
    "ycol = ['meantemp']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(feature_set)\n",
    "yscaled_data = scaler.fit_transform(feature_set[:,out_feature_index].reshape(-1,1))\n",
    "# scaler expects 2d --> needs reshape\n",
    "# The -1 in the first dimension indicates that the resulting array should have a size that is automatically determined to ensure that all the elements are included.\n",
    "# The 1 in the second dimension indicates that the resulting array should have a single column.\n",
    "# The purpose of this reshaping is to convert a 1-dimensional array (shape (n,)) into a 2-dimensional array with a single column (shape (n, 1)).\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(yscaled_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting of Hyperparameters\n",
    "\n",
    "Input size represents the number of features.\n",
    "\n",
    "Sequence length (x) refers to of x days worth of data prior to the present value.\n",
    "\n",
    "We use 2 layers of LSTM of size 256 nodes. Learning rate is set to 0.001. Batch size is set to 16.\n",
    "\n",
    "The number of epochs (20) defines the number times that the learning algorithm will work through the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "input_size = 4   # num_features\n",
    "sequence_length = 30   #blocksize\n",
    "\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the data into training data and validation data with a 80:20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 30, 4)\n",
      "(1125, 1)\n",
      "(289, 30, 4)\n",
      "(289, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_data,y_train_data=[],[]\n",
    "x_val_data,y_val_data=[],[]\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    if i < int(1444*0.8):\n",
    "        x_train_data.append(scaled_data[i - sequence_length : i,  :])\n",
    "        y_train_data.append(yscaled_data[i])\n",
    "        \n",
    "    else:\n",
    "        x_val_data.append(scaled_data[i - sequence_length : i,  :])\n",
    "        y_val_data.append(yscaled_data[i])        \n",
    "    \n",
    "x_train_data,y_train_data=np.array(x_train_data),np.array(y_train_data)\n",
    "x_val_data,y_val_data=np.array(x_val_data),np.array(y_val_data)\n",
    "\n",
    "print(x_train_data.shape)\n",
    "print(y_train_data.shape)\n",
    "print(x_val_data.shape)\n",
    "print(y_val_data.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LSTM Model\n",
    "\n",
    "First, we inherit the nn.Module within PyTorch which is a class created for general neural networks to create the LSTM class.\n",
    "\n",
    "Then, the initialisation method is created.\n",
    "\n",
    "super calls the initialization method of parents class nn.Module. It takes in two parameters: the first is the subclass, and the second parameter is an object that is an instance of that subclass\n",
    "\n",
    "The LSTM layers are created:\n",
    "batch_first --> If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature)\n",
    "bidirectional --> If True, becomes a bidirectional LSTM\n",
    "\n",
    "A final fully connected layer is created. The hidden size needs to be multiplied by 2 because one layer going forward and the other going backward.\n",
    "\n",
    "Last, we define the forward pass method.\n",
    "\n",
    "We need to define the hidden state and cell state for each node to be sent to the LSTM which we instantiate as a 3D zero tensor.\n",
    "\n",
    "The number of layers need to be multiplied by 2 because one layer going forward and the other going backward but they are all concatenated for the same hidden state. x.size(0) is the number of examples in a batch size. \n",
    "\n",
    "We take the last output hidden layer of the LSTM and send it to the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))   # only output is used here\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is sent to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and Optimizer\n",
    "\n",
    "The MSELoss loss criterion is used as this is a regression problem where the output is a continuous value and we can use mean-squared-error to calculate the loss function. The model will aim to minimize the average squared distance between predicted and true values and penalizes larger errors more heavily than smaller errors due to the squaring operation.\n",
    "\n",
    "Adam (Adaptive Moment Estimation) optimizer adapts the learning rate for each parameter based on its gradients and past updates. It computes separate learning rates for different parameters, allowing the optimizer to adjust the learning rates dynamically during training. This helps with faster convergence during back propagation.\n",
    "\n",
    "parameters() function was from inhertied nn.Module class\n",
    "When a Parameter is associated with a module as a model attribute, it gets added to the parameter list automatically and can be accessed using the 'parameters' iterator. They are the weights and biases of the model for each layer and their connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the numpy arrays of data into tensors and load them into DataLoader to create batches.\n",
    "\n",
    "shuffle=False is used as we do not want to disrupt the temporal element of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(x_train_data, dtype=torch.float32)\n",
    "train_y = torch.tensor(y_train_data, dtype=torch.float32)\n",
    "val_x = torch.tensor(x_val_data, dtype=torch.float32)\n",
    "val_y = torch.tensor(y_val_data, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "val_dataset = TensorDataset(val_x, val_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function for training per epoch\n",
    "\n",
    "We use enumerate(training_loader) instead of iter(training_loader) so that we can track the batch index and do some intra-epoch reporting. Each data instance is an input + label pair.\n",
    "\n",
    "We need to zero the gradients for every batch to prevent gradient accumulation across different batches (only want to aggregate within a batch).\n",
    "\n",
    "outputs = model(batch_x) --> predictions are made for the batch \n",
    "\n",
    "loss = criterion(outputs.squeeze(), batch_y) --> loss is calculated\n",
    "\n",
    "loss.backward() --> computes the partial derivative of the output f with respect to each of the input variables.\n",
    "\n",
    "optimizer.step() --> adam step and updates the parameters\n",
    "\n",
    "Next, we retrieve the scalar cvalue of the loss function for the current batch and print out the loss every 10 batches.\n",
    "\n",
    "tb_x = epoch_index * len(train_loader) + index + 1: This line calculates the value for the x-axis of a scalar summary in TensorBoard. It combines the current epoch index (epoch_index), the total number of batches per epoch (len(train_loader)), and the current batch index (index + 1) to get a unique value that represents the progress of training.\n",
    "\n",
    "tb_writer.add_scalar('Loss/train', last_loss, tb_x): This line adds a scalar summary to a TensorBoard writer (tb_writer is an instance of SummaryWriter). It logs the training loss (last_loss) under the tag 'Loss/train' at the x-axis value specified by tb_x\n",
    "\n",
    "Finally, the running loss is reset for the next set of 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for index, data in enumerate(train_loader):\n",
    "\n",
    "        batch_x, batch_y = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item() \n",
    "        if index % 10 == 9: \n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {index + 1} loss: {last_loss}')\n",
    "            tb_x = epoch_index * len(train_loader) + index + 1 \n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0\n",
    "\n",
    "    return last_loss #last calculated batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning the Results\n",
    "\n",
    "First, the SummaryWriter is instantiated.\n",
    "\n",
    "We set model to training mode to make sure that:\n",
    "1. batch normalization or dropout layers are in effect\n",
    "2. enables autograd tracking, which means that the model keeps track of the operations performed during the forward pass to calculate gradients during backpropagation. This allows the model to update its parameters based on the computed gradients.\n",
    "\n",
    "We use the training function to run for one epoch.\n",
    "\n",
    "The model is set back to evaluation mode as gradients are unnecessary for reporting of results on validation set.\n",
    "\n",
    "We then repeat the process of calculating the error and reporting it once for one epoch and also the average loss for the training set.\n",
    "\n",
    "We can log the results into Tensorboard and also save the model's state when it produces a new best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pirey/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.06505422201007605\n",
      "  batch 20 loss: 0.039954711124300955\n",
      "  batch 30 loss: 0.1174403615295887\n",
      "  batch 40 loss: 0.14612041059881448\n",
      "  batch 50 loss: 0.10406408119015395\n",
      "  batch 60 loss: 0.11511027179658413\n",
      "  batch 70 loss: 0.0659427233505994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pirey/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/pirey/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train 0.0659427233505994 validation 0.038318932056427\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.06025623129680753\n",
      "  batch 20 loss: 0.03869416220113635\n",
      "  batch 30 loss: 0.04256501779891551\n",
      "  batch 40 loss: 0.03423872496932745\n",
      "  batch 50 loss: 0.019244837295264005\n",
      "  batch 60 loss: 0.02632399518042803\n",
      "  batch 70 loss: 0.009874309680890292\n",
      "LOSS: train 0.009874309680890292 validation 0.015189691446721554\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.010655763489194215\n",
      "  batch 20 loss: 0.012897711526602507\n",
      "  batch 30 loss: 0.02131158858537674\n",
      "  batch 40 loss: 0.01826212229207158\n",
      "  batch 50 loss: 0.01950896279886365\n",
      "  batch 60 loss: 0.0140685913618654\n",
      "  batch 70 loss: 0.006801265082322061\n",
      "LOSS: train 0.006801265082322061 validation 0.015825098380446434\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.009535149671137333\n",
      "  batch 20 loss: 0.011040335847064852\n",
      "  batch 30 loss: 0.01451568838674575\n",
      "  batch 40 loss: 0.022474946500733495\n",
      "  batch 50 loss: 0.016125468839891254\n",
      "  batch 60 loss: 0.03816885445266962\n",
      "  batch 70 loss: 0.01298665440408513\n",
      "LOSS: train 0.01298665440408513 validation 0.04259011521935463\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.009292450733482838\n",
      "  batch 20 loss: 0.013402079557999968\n",
      "  batch 30 loss: 0.03235206203535199\n",
      "  batch 40 loss: 0.026865231059491634\n",
      "  batch 50 loss: 0.01388406245969236\n",
      "  batch 60 loss: 0.007868486549705267\n",
      "  batch 70 loss: 0.006465537199983373\n",
      "LOSS: train 0.006465537199983373 validation 0.014636117964982986\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.007338491850532591\n",
      "  batch 20 loss: 0.005587120947893709\n",
      "  batch 30 loss: 0.006495428783819079\n",
      "  batch 40 loss: 0.005700560798868537\n",
      "  batch 50 loss: 0.006761544669279829\n",
      "  batch 60 loss: 0.005854564241599291\n",
      "  batch 70 loss: 0.005545456981053576\n",
      "LOSS: train 0.005545456981053576 validation 0.005795636679977179\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.00593968756729737\n",
      "  batch 20 loss: 0.003574601619038731\n",
      "  batch 30 loss: 0.007357993104960769\n",
      "  batch 40 loss: 0.006946342438459397\n",
      "  batch 50 loss: 0.010356611426686868\n",
      "  batch 60 loss: 0.014057574374601246\n",
      "  batch 70 loss: 0.008381233573891223\n",
      "LOSS: train 0.008381233573891223 validation 0.01821296662092209\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.010400621918961406\n",
      "  batch 20 loss: 0.00733191471081227\n",
      "  batch 30 loss: 0.007028552913106978\n",
      "  batch 40 loss: 0.0076141661498695615\n",
      "  batch 50 loss: 0.007288552494719625\n",
      "  batch 60 loss: 0.009472198144067079\n",
      "  batch 70 loss: 0.006010222731856629\n",
      "LOSS: train 0.006010222731856629 validation 0.007132428232580423\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.006152182188816369\n",
      "  batch 20 loss: 0.00520134293474257\n",
      "  batch 30 loss: 0.010161658888682723\n",
      "  batch 40 loss: 0.008529812400229275\n",
      "  batch 50 loss: 0.007316235173493624\n",
      "  batch 60 loss: 0.008294950379058719\n",
      "  batch 70 loss: 0.005901897605508566\n",
      "LOSS: train 0.005901897605508566 validation 0.014685599133372307\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.005738414521329105\n",
      "  batch 20 loss: 0.004231503419578076\n",
      "  batch 30 loss: 0.005802724091336131\n",
      "  batch 40 loss: 0.004063441825564951\n",
      "  batch 50 loss: 0.007237200037343428\n",
      "  batch 60 loss: 0.007935833316878416\n",
      "  batch 70 loss: 0.005495466658612713\n",
      "LOSS: train 0.005495466658612713 validation 0.016688402742147446\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.008751521818339825\n",
      "  batch 20 loss: 0.006886882428079844\n",
      "  batch 30 loss: 0.017772192740812896\n",
      "  batch 40 loss: 0.016787559934891762\n",
      "  batch 50 loss: 0.011745822429656983\n",
      "  batch 60 loss: 0.013618399319238961\n",
      "  batch 70 loss: 0.010675555747002363\n",
      "LOSS: train 0.010675555747002363 validation 0.019021129235625267\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.010889262473210693\n",
      "  batch 20 loss: 0.007508127670735121\n",
      "  batch 30 loss: 0.006032766192220151\n",
      "  batch 40 loss: 0.007271857734303921\n",
      "  batch 50 loss: 0.00821598838083446\n",
      "  batch 60 loss: 0.011562160961329938\n",
      "  batch 70 loss: 0.006551228952594102\n",
      "LOSS: train 0.006551228952594102 validation 0.00799091998487711\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.006218292796984315\n",
      "  batch 20 loss: 0.006478675361722708\n",
      "  batch 30 loss: 0.014207909349352121\n",
      "  batch 40 loss: 0.01040149931795895\n",
      "  batch 50 loss: 0.008299785666167736\n",
      "  batch 60 loss: 0.011255030008032917\n",
      "  batch 70 loss: 0.0060738127678632734\n",
      "LOSS: train 0.0060738127678632734 validation 0.011027049273252487\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.005894102808088064\n",
      "  batch 20 loss: 0.004835121100768447\n",
      "  batch 30 loss: 0.005077256704680622\n",
      "  batch 40 loss: 0.004158933518920094\n",
      "  batch 50 loss: 0.0061878846026957035\n",
      "  batch 60 loss: 0.007860125933075324\n",
      "  batch 70 loss: 0.007791085517965257\n",
      "LOSS: train 0.007791085517965257 validation 0.03205559030175209\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.014607780007645487\n",
      "  batch 20 loss: 0.011150330957025289\n",
      "  batch 30 loss: 0.020390444132499397\n",
      "  batch 40 loss: 0.019920725049450992\n",
      "  batch 50 loss: 0.010643145721405744\n",
      "  batch 60 loss: 0.010303693404421211\n",
      "  batch 70 loss: 0.008187570795416831\n",
      "LOSS: train 0.008187570795416831 validation 0.009073317050933838\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.007154594361782074\n",
      "  batch 20 loss: 0.005421102908439934\n",
      "  batch 30 loss: 0.007522092992439866\n",
      "  batch 40 loss: 0.007714175374712795\n",
      "  batch 50 loss: 0.009514729829970748\n",
      "  batch 60 loss: 0.012615934759378434\n",
      "  batch 70 loss: 0.006250507530057803\n",
      "LOSS: train 0.006250507530057803 validation 0.008223822340369225\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.006292441440746188\n",
      "  batch 20 loss: 0.00554408059688285\n",
      "  batch 30 loss: 0.006950463261455297\n",
      "  batch 40 loss: 0.004778794175945222\n",
      "  batch 50 loss: 0.00917026263778098\n",
      "  batch 60 loss: 0.004901773325400427\n",
      "  batch 70 loss: 0.0065215454204007985\n",
      "LOSS: train 0.0065215454204007985 validation 0.010125678032636642\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.007304843608289957\n",
      "  batch 20 loss: 0.0043996616266667845\n",
      "  batch 30 loss: 0.011537509644404054\n",
      "  batch 40 loss: 0.009889210225082934\n",
      "  batch 50 loss: 0.007659069029614329\n",
      "  batch 60 loss: 0.009164112689904868\n",
      "  batch 70 loss: 0.009664224507287145\n",
      "LOSS: train 0.009664224507287145 validation 0.020030587911605835\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.011500229104422032\n",
      "  batch 20 loss: 0.007337915105745196\n",
      "  batch 30 loss: 0.005961624369956553\n",
      "  batch 40 loss: 0.006745553412474692\n",
      "  batch 50 loss: 0.007128612906672061\n",
      "  batch 60 loss: 0.009099951467942447\n",
      "  batch 70 loss: 0.005779457156313584\n",
      "LOSS: train 0.005779457156313584 validation 0.007780218031257391\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.006024610111489892\n",
      "  batch 20 loss: 0.005801791790872812\n",
      "  batch 30 loss: 0.012623540265485645\n",
      "  batch 40 loss: 0.00938949508126825\n",
      "  batch 50 loss: 0.007622249703854322\n",
      "  batch 60 loss: 0.011522934958338737\n",
      "  batch 70 loss: 0.006323112227255479\n",
      "LOSS: train 0.006323112227255479 validation 0.006878549233078957\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/Delhi_temp_trainer_{timestamp}')\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'EPOCH {epoch_number + 1}:')\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer) \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for index, vdata in enumerate(val_loader):\n",
    "        vbatch_x, vbatch_y = vdata\n",
    "        voutputs = model(vbatch_x)\n",
    "        vloss = criterion(voutputs.squeeze(), vbatch_y)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (index + 1)\n",
    "    print(f'LOSS: train {avg_loss} validation {avg_vloss}')\n",
    "    \n",
    "    '''\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'model_{timestamp}_{epoch_number}'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    '''\n",
    "    epoch_number += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, the loss decreases as the model gets trained over more epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the MSE of Model\n",
    "\n",
    "As this is for evaluation, it is not necessary to optimise with gradients as this will waste time and thus we use torch.no_grad()\n",
    "\n",
    "Here, we need to unscale the data to calculate the actual MSE of the results and thus we use the inverse_transform function of the scaler.\n",
    "\n",
    "'loss' contains the sum of MSE values of all the outputs within the batch\n",
    "\n",
    "Sum the loss over all the batches and devide by the number of samples.\n",
    "\n",
    "The overall MSE is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mse(loader,model): \n",
    "    if loader == train_loader: \n",
    "        print(\"checking mse on training data\")\n",
    "    else:\n",
    "        print(\"checking mse on test data\")\n",
    "    overall_loss = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for x,y in loader:\n",
    "            x= x.to(device=device)\n",
    "            y= y.to(device=device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            unscaled_outputs = scaler.inverse_transform(np.array(outputs))\n",
    "            unscaled_y = scaler.inverse_transform(np.array(y))\n",
    "            \n",
    "            unscaled_outputs = torch.from_numpy(unscaled_outputs)\n",
    "            unscaled_y = torch.from_numpy(unscaled_y)            \n",
    "            \n",
    "            loss = F.mse_loss(unscaled_outputs,unscaled_y,reduction='sum')\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            num_samples += x.size(0)\n",
    "\n",
    "        mse = overall_loss / num_samples\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking mse on training data\n",
      "MSE: 4.5990\n",
      "checking mse on test data\n",
      "MSE: 4.9128\n"
     ]
    }
   ],
   "source": [
    "check_mse(train_loader,model)\n",
    "check_mse(val_loader,model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error is relatively low for the bi-directional LSTM model, showing that it works for this time series problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
